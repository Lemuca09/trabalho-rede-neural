# -*- coding: utf-8 -*-
"""PrevisaoAcoes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aPwEvSpty99XAXxz9JWHqAFNgz4Xksxf
"""

import os
import json
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

"""**Configurações**"""

TICKERS = ['AAPL', 'MSFT', 'GOOGL']
# TICKERS = ['GOOGL']
# TICKERS = ['NVDA', 'AMZN', 'META', 'ODD'] # Exemplo onde ODD tende a ser contrário aos outros valores no PCA
# TICKERS = ['NVDA', 'AMZN', 'META']
WINDOW_SIZE = 12  # 12 semanas (~3 meses)
EPOCHS = 300 # 300
BATCH_SIZE = 4

TRAIN = False # Controle para se for necessário retreinar

FUTURE_STEPS = 4  # número de semanas a prever

MODEL_PATH = "modelo_lstm_portfolio_acoes.h5"
HISTORY_PATH = "history_lstm.json"

"""**Baixar dados semanais dos últimos 3 anos**"""

dfs = []
for ticker in TICKERS:
    data = yf.Ticker(ticker).history(period="3y", interval="1wk")['Close']
    data.name = ticker
    dfs.append(data)

len(data), data

"""**Combinar e limpar**"""

df = pd.concat(dfs, axis=1)
df.dropna(inplace=True)

"""**Criar valor médio do portfólio**"""

df['Portfolio'] = df.mean(axis=1)
portfolio_values = df['Portfolio'].values

"""**Padronizar (z-score)**"""

mean = np.mean(portfolio_values)
std = np.std(portfolio_values)
portfolio_scaled = (portfolio_values - mean) / std

mean, std, len(portfolio_scaled)

"""**Janela deslizante**"""

X, y = [], []
for i in range(len(portfolio_scaled) - WINDOW_SIZE): # ~145
    X.append(portfolio_scaled[i:i + WINDOW_SIZE])
    y.append(portfolio_scaled[i + WINDOW_SIZE])
X = np.array(X)
y = np.array(y)

len(X) # 158 Semanas - Window_Size=12, logo, 146

"""**Reshape para LSTM: (samples/batch_size, timesteps, features)**"""

X = X.reshape((X.shape[0], X.shape[1], 1)) # 102/44 sequências no batch, 12 Semanas, 1 Valor/Preço
X.shape

"""**Dividir treino/teste**"""

split = int(len(X) * 0.7) # 0.8
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

X[:split].shape, X[split:].shape      # 102/44 sequências, cada uma com 12 passos temporais, cada passo tem 1 feature.

"""**Dividir treino/teste** E **Treinar**"""

if os.path.exists(MODEL_PATH) and TRAIN != True:
    print("🔁 Carregando modelo salvo...")
    model = tf.keras.models.load_model(MODEL_PATH)
    model.compile(optimizer='adam', loss=tf.keras.losses.Huber(0.8), metrics=['mse','mae', 'mape'])
    with open(HISTORY_PATH, "r") as f:
        loss_data = json.load(f)
else:
    print("🔁 Treinando novo modelo...")
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64, activation='tanh', input_shape=(WINDOW_SIZE, 1)),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(64, activation='elu'),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(1)
    ])

    model.compile(optimizer='adam', loss=tf.keras.losses.Huber(0.8), metrics=['mse','mae', 'mape'])
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        verbose=1
    )
    model.save(MODEL_PATH)
    print(f"✅ Modelo salvo em {MODEL_PATH}")

    # Salvar histórico - Loss
    with open(HISTORY_PATH, "w") as f:
        json.dump(history.history, f)
    print(f"✅ Histórico salvo em {HISTORY_PATH}")

"""**Prever e Inverter Padronização**"""

predictions = model.predict(X_test).flatten() # Finalmente usa os Valores de Test para fazer as Previsões, passa todas as janelas de entrada (X) para o modelo LSTM e depois transforma um array 2D (shape: (n, 1)) para 1D
predictions_real = predictions * std + mean
y_test_real = y_test * std + mean


print(abs(history.history['val_loss'][-1]))
print(abs(history.history['val_mae'][-1]))
print(abs(history.history['val_mse'][-1]))
print(abs(history.history['val_mape'][-1]))
print()
print(abs(history.history['val_mape'][-1]- 100)) # Valor de Acerto em % do Modelo na Última Época
print(abs(min(history.history['val_mape']) - 100)) # Valor de Acerto em % do Modelo na Época com o valor de Erro mais Baixo. Onde o Peso e Bias está melhor ajustado.

"""## Visualizando o conjunto de teste (test set). O modelo foi treinado com 70% dos dados (split = int(len(X) * 0.7)), e o restante (30%) é usado para teste e plot. Isso representa cerca de:

##156 semanas ≈ **3 anos**

## 30% de 156 ≈ 47 semanas, ou seja quase **12 meses**.
"""

dates = df.index[-len(y_test_real):]  # pega as datas correspondentes ao conjunto teste


plt.figure(figsize=(12, 5))
plt.style.use('dark_background')
plt.plot(dates, y_test_real, label='Valor real (Média das Ações)') if len(TICKERS) >= 2 else None
# for ticker in TICKERS:
#   plt.plot(dates, df[ticker][-len(y_test_real):].values, label=ticker) if len(TICKERS) > 1 else None
plt.plot(dates, predictions_real, label='Previsão da rede', linestyle='--')
plt.title(f'Previsão do Portfólio com Dados Mensais {len(dates)/4}')
plt.xlabel('Data')
plt.ylabel('Valor ($)')
plt.legend()
plt.grid(True)
plt.show()


dates[0], dates[-1]

print(f"{len(dates)} semanas de dados, logo {len(dates)/4} Meses") # Lembrar da Subtração com Window Size

"""**Sugestão de Decisão de Compra/Espera/Venda das Ações**"""

threshold = 0.015  # 1.5% de variação para decidir compra/venda

index_max = np.argmax(y_test_real)
index_min = np.argmin(y_test_real)

valor_max_real = y_test_real[index_max]
valor_max_previsto = predictions_real[index_max]
data_max = dates[index_max]

valor_min_real = y_test_real[index_min]
valor_min_previsto = predictions_real[index_min]
data_min = dates[index_min]

print(f"📈 Ação mais cara (Portfólio - {' e '.join(TICKERS)}):")
print(f"Data: {data_max.strftime('%Y-%m-%d')}")
print(f"Valor real   : ${valor_max_real:.2f}")
print(f"Valor previsto: ${valor_max_previsto:.2f}")
erro = valor_max_real - valor_max_previsto
print(f"Erro de: ${abs(erro):.2f}\n")

print(f"📉 Ação mais barata (Portfólio - {' e '.join(TICKERS)}):")
print(f"Data: {data_min.strftime('%Y-%m-%d')}")
print(f"Valor real   : ${valor_min_real:.2f}")
print(f"Valor previsto: ${valor_min_previsto:.2f}")
erro2 = valor_min_real - valor_min_previsto
print(f"Erro de: ${abs(erro2):.2f}\n")

decisoes = []
for i in range(len(predictions_real) - 1):
    atual = predictions_real[i]
    futuro = predictions_real[i + 1]
    delta = (futuro - atual) / atual

    if i == 0:
        delta = 0
        decisao = "🟢 Compra"
    else:
        if delta > threshold:
            decisao = "🟢 Comprar"
        elif delta < -threshold:
            decisao = "🔴 Vender"
        else:
            decisao = "⚪️ Esperar"

    decisoes.append((dates[i].strftime('%Y-%m-%d'), atual, delta * 100, decisao))

print(f"💰 De Acordo com o modelo de Previsão:\n")
print(f"{'Data':<12} {'Valor Atual':<14} {'Δ%':<8} {'Decisão'}")
for data, atual, delta, decisao in decisoes:
    print(f"{data:<12} ${atual:<13.2f} {delta:>+6.2f}%   {decisao}")

capital_inicial = 1000.0
dinheiro = capital_inicial
acoes = 0
historico_compras = []

for i, (data_str, atual, delta_pct, decisao) in enumerate(decisoes):
    delta = delta_pct / 100

    if i == 0 and dinheiro >= atual:
        maximo_acoes = int(dinheiro / atual)
        if maximo_acoes > 0:
            custo = maximo_acoes * atual
            dinheiro -= custo
            acoes += maximo_acoes
            historico_compras.append((data_str, maximo_acoes, atual))
        continue

    if decisao == "🟢 Comprar" and dinheiro >= atual:
        excesso = delta - threshold
        fator = excesso / threshold  # quanto acima do threshold
        fator = max(1.0, fator)  # no mínimo 1 ação

        fator = min((excesso / threshold), 5)  # fator de escala, limitado a 5
        orcamento = dinheiro * (fator / 5)  # usar parte proporcional do dinheiro
        maximo_acoes = int(orcamento / atual)  # limitar para evitar exageros

        if maximo_acoes > 0 and dinheiro >= maximo_acoes:
            custo = maximo_acoes * atual
            dinheiro -= custo
            acoes += maximo_acoes
            historico_compras.append((data_str, maximo_acoes, atual))

    elif decisao == "🔴 Vender" and acoes > 0:
        dinheiro += acoes * atual
        historico_compras.append((data_str, -acoes, atual))  # registro da venda
        acoes = 0

preco_final = predictions_real[-1]
valor_final = dinheiro + acoes * preco_final
lucro = valor_final - capital_inicial

if historico_compras:
    print("\n📋 Registro de Transações:")
    print(f"{'Data':<12} {'Ações':<10} {'Valor Unitário':<15} {'Total'}")
    for data, qtd, valor in historico_compras:
        total = abs(qtd) * valor
        tipo = "Compra" if qtd > 0 else "Venda"
        print(f"{data:<12} {qtd:<10} ${valor:<14.2f} ${total:.2f} ({tipo})")
else:
    print("\n⚠️ Nenhuma transação foi realizada.")

print("\n📊 Resultados finais da simulação:")
print(f"Dinheiro restante (Não investido): ${dinheiro:.2f}")
print(f"Ações restantes: {acoes}")
print(f"Preço final da ação: ${preco_final:.2f}")
print(f"Valor final da carteira: ${valor_final:.2f}")
print(f"Lucro total: ${lucro:.2f} - {(lucro/capital_inicial)*100:.2f}%")

"""**Passos Futuro**"""

last_window = portfolio_scaled[-WINDOW_SIZE:].tolist()
future_predictions_scaled = []

for _ in range(FUTURE_STEPS):
    input_array = np.array(last_window[-WINDOW_SIZE:]).reshape(1, WINDOW_SIZE, 1) # Como é LSTM, redimensiona para o formato esperado
    next_pred = model.predict(input_array)[0, 0]
    future_predictions_scaled.append(next_pred)
    last_window.append(next_pred)

future_predictions_real = [p * std + mean for p in future_predictions_scaled] # Desnormaliza as previsões para escala original

predictions_str = ', '.join([str(p) for p in future_predictions_real])
print(f"\nValores Previstos: {np.array(predictions_str)}")

plt.figure(figsize=(12, 5))
plt.style.use('dark_background')
plt.plot(df.index[-len(y_test_real):], y_test_real, label='Valor real (Média das Ações)')
plt.plot(df.index[-len(predictions_real):], predictions_real, label='Previsão da rede', linestyle='--')
plt.plot(pd.date_range(df.index[-1], periods=FUTURE_STEPS+1, freq='W')[1:], future_predictions_real, label=f'Previsão +{FUTURE_STEPS} semanas', linestyle='-.', color="red")
plt.title(f'Previsão do Portfólio com Extensão Futura (11 Meses + {FUTURE_STEPS} Semanas)')
plt.xlabel('Data')
plt.ylabel('Valor ($)')
plt.legend()
plt.grid(True)
plt.show()

"""####Esse gráfico mostra a performance do modelo LSTM ao longo dos **3 anos**, e não só no conjunto de teste."""

all_predictions = model.predict(X).flatten() # Passa todas as janelas de entrada (X) para o modelo LSTM e depois transforma um array 2D (shape: (n, 1)) para 1D
all_predictions_real = all_predictions * std + mean
all_y_real = y * std + mean

all_dates = df.index[WINDOW_SIZE:]  # compensar janela deslizante

plt.figure(figsize=(12, 6))
plt.style.use('dark_background')
plt.plot(all_dates, all_y_real, label='Valor real (Média das Ações)')
plt.plot(all_dates, all_predictions_real, label='Previsão do modelo', linestyle='--')
plt.title('Previsão do Portfólio nos Últimos 3 Anos (Treino + Teste)')
plt.xlabel('Data')
plt.ylabel('Valor ($)')
plt.legend()
plt.grid(True)
plt.show()

len(all_dates)

"""**Visualização do Loss**"""

if history:
    loss_data = history.history
elif os.path.exists(HISTORY_PATH):
    with open(HISTORY_PATH, "r") as f:
        loss_data = json.load(f)
else:
    loss_data = None

if loss_data:
    plt.plot(loss_data['loss'], label='Training Loss')
    plt.plot(loss_data['val_loss'], label='Validation Loss')
    plt.title('Loss por Época')
    plt.xlabel('Épocas')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()
else:
    print("⚠️ Nenhum histórico de loss encontrado.")

"""**Inicializar PCA**"""

scaler = StandardScaler()
if 'Portfolio' in df.columns:
  df = df.drop("Portfolio",axis=1)
scaled_df = scaler.fit_transform(df)

pca = PCA()
pca.fit(scaled_df)
explained_variance = pca.explained_variance_ratio_ # Variância explicada por componente, quanto da variância total cada componente principal explica

len(scaled_df), explained_variance,scaled_df, pca.components_, df

"""**Componentes principais (pesos das ações)**"""

components = pca.components_
pca_df = pd.DataFrame(components.T,
                      columns=[f'PC{i+1}' for i in range(len(components))],
                      index=df.columns)

"""**Mostrar a contribuição de cada ação no 1ª componente principal**"""

print("Contribuição de cada ação no 1º componente principal (PC1):")
print(pca_df['PC1'].sort_values(ascending=False))

"""**Mostrar a contribuição de cada ação no 2ª componente principal**"""

if 'PC2' in pca_df.columns:
  print("Contribuição de cada ação no 2º componente principal (PC2):")
  print(pca_df['PC2'].sort_values(ascending=False))

loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(len(df.columns))], index=df.columns)

def interpretar_pca(loadings):
    analises = []

    pc1_abs = loadings['PC1'].abs().sort_values(ascending=False)
    mais_influente = pc1_abs.index[0]
    menos_influente = pc1_abs.index[-1]

    analises.append(f"💡 Análise PC1\n\n⏩ O componente principal 1 (PC1) representa a direção de maior variância dos dados, ou seja, a combinação linear que mais resume a variação dos preços das ações.\n\n🔀 {mais_influente} é o ativo que mais influencia, enquanto {menos_influente} interfere menos.")

    if 'PC2' in loadings.columns:
        pc2 = loadings['PC2']
        positivos = pc2[pc2 > 0].sort_values(ascending=False)
        negativos = pc2[pc2 < 0].sort_values()

        if not positivos.empty and not negativos.empty:
            analises.append(
                f"\n💡 Análise PC2\n\n"
                f"⏩ {positivos.index[0]} = {positivos.iloc[0]:.3f}: contribui positivamente para PC2.\n\n"
                f"⏩ {' e '.join(negativos.index)} negativos: vão na direção oposta de {positivos.index[0]} neste componente.\n\n"
                f"🔁 Isso sugere que quando {positivos.index[0]} se desvia da média geral, "
                f"{' e '.join(negativos.index)} tendem a ir na direção contrária."
            )

    return "\n\n".join(analises)

print(interpretar_pca(loadings))